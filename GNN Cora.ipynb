{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import normalize,toy_data,norm_embed,nmi_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution, InnerProduct\n",
    "from utils import norm_embed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, nfeat, nhid, ndim, mu0, sigma0, fixed):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(batch_size, nfeat, nhid, mu0, sigma0, scale=False)\n",
    "        self.fixed = fixed\n",
    "        if self.fixed:\n",
    "            self.embeddings = GraphConvolution(batch_size, nhid, 2*ndim, mu0, sigma0, scale=True)\n",
    "            self.reconstructions = InnerProduct(2*ndim)\n",
    "        else:\n",
    "            self.embeddings = GraphConvolution(batch_size, nhid, 4 * ndim, mu0, sigma0, scale=True)\n",
    "            self.reconstructions = InnerProduct(4*ndim)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "\n",
    "        if self.fixed:\n",
    "            mu = F.relu(self.reconstructions(x))\n",
    "            return mu, x\n",
    "        else:\n",
    "            lr1, lr2 = torch.chunk(x, chunks=2, dim=2)\n",
    "            mu = F.relu(self.reconstructions(lr1))\n",
    "            sigma = F.relu(self.reconstructions(lr2))\n",
    "            return mu, sigma, x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False,precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=426, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=500,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=10e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--ndim', type=int, default=2,\n",
    "                    help='Embeddings dimension.')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cite=False,cora=False,pub=False):\n",
    "    \n",
    "    if cite:\n",
    "        df1 = pd.read_csv('data/citeseer/citeseer.edges')\n",
    "        df2 = pd.read_csv('data/citeseer/citeseer.node_labels')\n",
    "        G = nx.from_pandas_edgelist(df1, 'u', 'v','weight',create_using=nx.DiGraph())\n",
    "        adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "        labels = df2.to_numpy()[:,-1].reshape(-1,1)\n",
    "        labels -= 1\n",
    "    \n",
    "    elif cora:\n",
    "        df1 = pd.read_csv('data/cora/cora.edges')\n",
    "        df2 = pd.read_csv('data/cora/cora.node_labels')\n",
    "        G = nx.from_pandas_edgelist(df1, 'u', 'v','weight',create_using=nx.DiGraph())\n",
    "        adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "        labels = df2.to_numpy()[:,-1].reshape(-1,1)\n",
    "        labels -= 1\n",
    "    \n",
    "    elif pub:\n",
    "        df1 = pd.read_csv('data/pubmed/pubmed.edges')\n",
    "        df2 = pd.read_csv('data/pubmed/pubmed.node_labels')\n",
    "        G = nx.from_pandas_edgelist(df1, 'u', 'v',create_using=nx.DiGraph())\n",
    "        adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "        \n",
    "    return adj_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, labels = load_data(cora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdApprox(adj,dim,relu=False):\n",
    "    adj = torch.FloatTensor(adj[0])\n",
    "    U, S, Vh = torch.linalg.svd(adj)\n",
    "    mu = torch.matmul(torch.matmul(U[:, :dim], torch.diag(S[:dim])), Vh[:dim, :])\n",
    "    \n",
    "    embedx = torch.matmul(U[:, :dim],torch.diag(torch.pow(S[:dim], 0.5)))\n",
    "    embedy = torch.transpose(torch.matmul(torch.diag(torch.pow(S[:dim], 0.5)),Vh[:dim, :]),0,1)\n",
    "    \n",
    "    criterion = torch.nn.GaussianNLLLoss()\n",
    "    if relu:\n",
    "        crt = torch.nn.ReLU()\n",
    "        mu = crt(mu)\n",
    "    mse = torch.nn.MSELoss()\n",
    "    mseloss = mse(torch.flatten(mu), torch.flatten(adj))\n",
    "    sig = torch.sqrt(mseloss)\n",
    "    sigma = sig * torch.ones(adj.shape)\n",
    "    loss = criterion(torch.flatten(adj), torch.flatten(mu), torch.flatten(torch.square(sigma)))\n",
    "    \n",
    "    return mu,sigma,loss.item(),embedx,embedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphNeuralNet(adj,dim,fixed=False,new=True,features=None,sig_fix=None):\n",
    "    \n",
    "    # Set the random seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        \n",
    "    args.ndim = dim\n",
    "    \n",
    "    adj_norm = normalize(adj)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj)).cuda()\n",
    "    \n",
    "    # loss function\n",
    "    criterion = torch.nn.GaussianNLLLoss()\n",
    "    \n",
    "    # NULL Model\n",
    "    mu0 = adj.mean()*torch.ones(adj.shape[1:]).cuda()\n",
    "    sigma0 = adj.std()*torch.ones(adj.shape[1:]).cuda()\n",
    "    with torch.no_grad():\n",
    "        loss0 = criterion(torch.flatten(adj), torch.flatten(mu0), torch.flatten(torch.square(sigma0)))\n",
    "    \n",
    "    if new:\n",
    "        if fixed:\n",
    "            #svd features\n",
    "            svd_mu,svd_sig,svd_loss,svdembedx,svdembedy = svdApprox(adj=adj.cpu(),dim=dim)\n",
    "            features = torch.cat((svdembedx,svdembedy),dim=1)\n",
    "        if not fixed:\n",
    "            sig_flex = torch.ones(features.shape).cuda()*torch.sqrt(sig_fix/dim).cuda()\n",
    "            features = torch.cat((features,sig_flex),dim=1).cuda()\n",
    "        features = features.unsqueeze(dim=0)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Model and optimizer  \n",
    "        \n",
    "    model = GNN(batch_size=adj.shape[0],\n",
    "                nfeat=adj.shape[1],\n",
    "                nhid=adj.shape[1],\n",
    "                ndim=args.ndim,\n",
    "                mu0=adj.mean(),\n",
    "                sigma0=adj.std(),\n",
    "                fixed=fixed)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        adj_norm = adj_norm.cuda()\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    \n",
    "    # NULL Model\n",
    "    mu0 = adj.mean()*torch.ones(adj.shape[1:]).cuda()\n",
    "    sigma0 = adj.std()*torch.ones(adj.shape[1:]).cuda()\n",
    "    with torch.no_grad():\n",
    "        loss0 = criterion(torch.flatten(adj), torch.flatten(mu0), torch.flatten(torch.square(sigma0)))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if fixed:\n",
    "            mu,lr = model(features, adj_norm)\n",
    "            with torch.no_grad():\n",
    "                mse = torch.nn.MSELoss()\n",
    "                mseloss = mse(torch.flatten(mu),torch.flatten(adj))\n",
    "                sig = torch.sqrt(mseloss)\n",
    "            sigma = sig * torch.ones(adj.shape,requires_grad=True).cuda()\n",
    "        else:\n",
    "            mu,sigma,lr = model(features, adj_norm)\n",
    "        \n",
    "        \n",
    "        loss = criterion(torch.flatten(adj), torch.flatten(mu), torch.flatten(torch.square(sigma))) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "            if fixed:\n",
    "                best_sig = sig\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_lr = lr\n",
    "                if fixed:\n",
    "                    best_sig = sig\n",
    "\n",
    "        if epoch == 0 or (epoch+1) % 100 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'loss: {:.8f}'.format(best_loss.item()))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    if fixed:\n",
    "        return mu,best_loss.item(),loss0,best_lr,best_sig\n",
    "    else:\n",
    "        return mu,best_loss.item(),loss0,best_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Sigma dim 100\n",
      "Epoch: 0001 loss: -3.38226867\n",
      "Epoch: 0100 loss: -4.28764772\n",
      "Epoch: 0200 loss: -5.63868999\n",
      "Epoch: 0300 loss: -5.63923168\n",
      "Epoch: 0400 loss: -5.63946438\n",
      "Epoch: 0500 loss: -5.63959122\n",
      "Optimization Finished!\n",
      "Total time elapsed: 81.7307s\n",
      "Epoch: 0001 loss: -5.63959122\n",
      "Epoch: 0100 loss: -5.64069843\n",
      "Epoch: 0200 loss: -5.64078140\n",
      "Epoch: 0300 loss: -5.64078140\n",
      "Epoch: 0400 loss: -5.64078140\n",
      "Epoch: 0500 loss: -5.64078140\n",
      "Optimization Finished!\n",
      "Total time elapsed: 81.1529s\n",
      "Flexible Sigma dim 100\n",
      "Epoch: 0001 loss: -5.64077997\n",
      "Epoch: 0100 loss: -6.83694696\n",
      "Epoch: 0200 loss: -6.86316633\n",
      "Epoch: 0300 loss: -6.87148619\n",
      "Epoch: 0400 loss: -6.87718344\n",
      "Epoch: 0500 loss: -6.88059473\n",
      "Optimization Finished!\n",
      "Total time elapsed: 79.2808s\n",
      "Epoch: 0001 loss: -6.88059473\n",
      "Epoch: 0100 loss: -6.88554573\n",
      "Epoch: 0200 loss: -6.88849020\n",
      "Epoch: 0300 loss: -6.89077091\n",
      "Epoch: 0400 loss: -6.89276743\n",
      "Epoch: 0500 loss: -6.89422274\n",
      "Optimization Finished!\n",
      "Total time elapsed: 80.3744s\n"
     ]
    }
   ],
   "source": [
    "for dim in [100]:\n",
    "    print(\"Fixed Sigma dim {}\".format(dim))\n",
    "\n",
    "    mu,loss,loss0,lr,sigma = GraphNeuralNet(adj=adj,dim=dim,fixed=True)\n",
    "    mu,loss,loss0,lr,sigma = GraphNeuralNet(adj=adj,dim=dim,fixed=True,new=False,features=lr.detach())\n",
    "\n",
    "    print(\"Flexible Sigma dim {}\".format(dim))\n",
    "    args.lr *= 0.1\n",
    "\n",
    "    mu,loss,loss0,lr = GraphNeuralNet(adj=adj,dim=dim,new=True,features=lr[0].detach(),sig_fix=sigma)\n",
    "    mu,loss,loss0,lr = GraphNeuralNet(adj=adj,dim=dim,new=False,features=lr.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1, lr2 = torch.chunk(lr, chunks=2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Train %\n",
    "\n",
    "train_percentage = .8\n",
    "    \n",
    "# Train set\n",
    "number_of_rows = lr1.shape[1]\n",
    "train_indices = np.random.choice(number_of_rows, size=int(train_percentage*number_of_rows), replace=False)\n",
    "val_indices = np.setdiff1d(np.arange(lr1.shape[1]),train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classify_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(classify_net, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        for i in range(4):\n",
    "            x1 = self.layer2(x)\n",
    "            x = x + x1\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(embed,labels,lr):\n",
    "\n",
    "    embed = torch.FloatTensor(embed)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    model = classify_net(embed.shape[1],32,7)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        embed = embed.cuda()\n",
    "        labels = labels.cuda()\n",
    "    \n",
    "\n",
    "    t_total = time.time()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr)\n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(100000):\n",
    "\n",
    "        t = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(embed)\n",
    "\n",
    "        train_output = output[train_indices,:]\n",
    "        train_labels = labels[train_indices,:]\n",
    "        \n",
    "        train_accuracy = torch.sum(torch.argmax(train_output,axis=1)==train_labels.reshape(1,-1))/train_labels.shape[0]\n",
    "\n",
    "        loss = criterion(output,labels.reshape(-1).long())\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Calculate Validation accuracy\n",
    "        with torch.no_grad():\n",
    "            val_output = output[val_indices,:]\n",
    "            val_labels = labels[val_indices,:]\n",
    "            val_accuracy = torch.sum(torch.argmax(val_output,axis=1)==val_labels.reshape(1,-1))/val_labels.shape[0]\n",
    "\n",
    "        # Print summary of training \n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_output = output\n",
    "            best_acc = train_accuracy\n",
    "            best_val_acc = val_accuracy\n",
    "            best_val_output = val_output\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_output = output\n",
    "                best_acc = train_accuracy\n",
    "                best_val_acc = val_accuracy\n",
    "                best_val_output = val_output\n",
    "\n",
    "        if epoch == 0 or (epoch+1) % 1000 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'Train Accuracy: {:.4f}'.format(best_acc.item()),\n",
    "                  'Validation Accuracy: {:.4f}'.format(best_val_acc.item()),\n",
    "                  'Loss: {:.8f}'.format(best_loss.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Train Accuracy: 0.0180 Validation Accuracy: 0.0092 Loss: 3.62335134 time: 0.0102s\n",
      "Epoch: 1000 Train Accuracy: 0.4885 Validation Accuracy: 0.4834 Loss: 1.34055340 time: 0.0085s\n",
      "Epoch: 2000 Train Accuracy: 0.6053 Validation Accuracy: 0.5683 Loss: 1.12345827 time: 0.0000s\n",
      "Epoch: 3000 Train Accuracy: 0.6524 Validation Accuracy: 0.6568 Loss: 1.02253735 time: 0.0000s\n",
      "Epoch: 4000 Train Accuracy: 0.6934 Validation Accuracy: 0.7048 Loss: 0.94226879 time: 0.0045s\n",
      "Epoch: 5000 Train Accuracy: 0.7221 Validation Accuracy: 0.7362 Loss: 0.88296026 time: 0.0000s\n",
      "Epoch: 6000 Train Accuracy: 0.7553 Validation Accuracy: 0.7546 Loss: 0.83789521 time: 0.0000s\n",
      "Epoch: 7000 Train Accuracy: 0.7669 Validation Accuracy: 0.7675 Loss: 0.79470003 time: 0.0025s\n",
      "Epoch: 8000 Train Accuracy: 0.7807 Validation Accuracy: 0.7823 Loss: 0.75969136 time: 0.0000s\n",
      "Epoch: 9000 Train Accuracy: 0.8015 Validation Accuracy: 0.8044 Loss: 0.72821128 time: 0.0100s\n",
      "Epoch: 10000 Train Accuracy: 0.8047 Validation Accuracy: 0.8210 Loss: 0.70301133 time: 0.0000s\n",
      "Epoch: 11000 Train Accuracy: 0.8181 Validation Accuracy: 0.8155 Loss: 0.68443048 time: 0.0000s\n",
      "Epoch: 12000 Train Accuracy: 0.8259 Validation Accuracy: 0.8192 Loss: 0.66585881 time: 0.0000s\n",
      "Epoch: 13000 Train Accuracy: 0.8292 Validation Accuracy: 0.8155 Loss: 0.65467781 time: 0.0000s\n",
      "Epoch: 14000 Train Accuracy: 0.8356 Validation Accuracy: 0.8524 Loss: 0.64385813 time: 0.0103s\n",
      "Epoch: 15000 Train Accuracy: 0.8449 Validation Accuracy: 0.8247 Loss: 0.63222736 time: 0.0041s\n",
      "Epoch: 16000 Train Accuracy: 0.8440 Validation Accuracy: 0.8413 Loss: 0.62760395 time: 0.0102s\n",
      "Epoch: 17000 Train Accuracy: 0.8467 Validation Accuracy: 0.8506 Loss: 0.62081939 time: 0.0000s\n",
      "Epoch: 18000 Train Accuracy: 0.8467 Validation Accuracy: 0.8598 Loss: 0.61177266 time: 0.0000s\n",
      "Epoch: 19000 Train Accuracy: 0.8532 Validation Accuracy: 0.8339 Loss: 0.60901886 time: 0.0000s\n",
      "Epoch: 20000 Train Accuracy: 0.8560 Validation Accuracy: 0.8506 Loss: 0.60495335 time: 0.0075s\n",
      "Epoch: 21000 Train Accuracy: 0.8610 Validation Accuracy: 0.8321 Loss: 0.59660095 time: 0.0091s\n",
      "Epoch: 22000 Train Accuracy: 0.8601 Validation Accuracy: 0.8542 Loss: 0.58930504 time: 0.0055s\n",
      "Epoch: 23000 Train Accuracy: 0.8661 Validation Accuracy: 0.8598 Loss: 0.58140934 time: 0.0126s\n",
      "Epoch: 24000 Train Accuracy: 0.8578 Validation Accuracy: 0.8801 Loss: 0.57588077 time: 0.0055s\n",
      "Epoch: 25000 Train Accuracy: 0.8675 Validation Accuracy: 0.8690 Loss: 0.57035089 time: 0.0090s\n",
      "Epoch: 26000 Train Accuracy: 0.8781 Validation Accuracy: 0.8764 Loss: 0.56209904 time: 0.0045s\n",
      "Epoch: 27000 Train Accuracy: 0.8680 Validation Accuracy: 0.8727 Loss: 0.56097108 time: 0.0050s\n",
      "Epoch: 28000 Train Accuracy: 0.8717 Validation Accuracy: 0.8782 Loss: 0.55274290 time: 0.0000s\n",
      "Epoch: 29000 Train Accuracy: 0.8740 Validation Accuracy: 0.8801 Loss: 0.55062753 time: 0.0060s\n",
      "Epoch: 30000 Train Accuracy: 0.8717 Validation Accuracy: 0.8782 Loss: 0.54836810 time: 0.0065s\n",
      "Epoch: 31000 Train Accuracy: 0.8726 Validation Accuracy: 0.8745 Loss: 0.54551321 time: 0.0000s\n",
      "Epoch: 32000 Train Accuracy: 0.8777 Validation Accuracy: 0.8856 Loss: 0.54287982 time: 0.0101s\n",
      "Epoch: 33000 Train Accuracy: 0.8786 Validation Accuracy: 0.8819 Loss: 0.54156721 time: 0.0105s\n",
      "Epoch: 34000 Train Accuracy: 0.8827 Validation Accuracy: 0.8690 Loss: 0.53869528 time: 0.0000s\n",
      "Epoch: 35000 Train Accuracy: 0.8777 Validation Accuracy: 0.8893 Loss: 0.53505343 time: 0.0000s\n",
      "Epoch: 36000 Train Accuracy: 0.8804 Validation Accuracy: 0.8690 Loss: 0.53189379 time: 0.0045s\n",
      "Epoch: 37000 Train Accuracy: 0.8804 Validation Accuracy: 0.8690 Loss: 0.53189379 time: 0.0090s\n",
      "Epoch: 38000 Train Accuracy: 0.8901 Validation Accuracy: 0.8690 Loss: 0.53046924 time: 0.0000s\n",
      "Epoch: 39000 Train Accuracy: 0.8910 Validation Accuracy: 0.9004 Loss: 0.52662086 time: 0.0101s\n",
      "Epoch: 40000 Train Accuracy: 0.8920 Validation Accuracy: 0.8856 Loss: 0.52525103 time: 0.0055s\n",
      "Epoch: 41000 Train Accuracy: 0.8920 Validation Accuracy: 0.8856 Loss: 0.52525103 time: 0.0101s\n",
      "Epoch: 42000 Train Accuracy: 0.8934 Validation Accuracy: 0.8967 Loss: 0.52367127 time: 0.0031s\n",
      "Epoch: 43000 Train Accuracy: 0.8846 Validation Accuracy: 0.8948 Loss: 0.52251714 time: 0.0102s\n",
      "Epoch: 44000 Train Accuracy: 0.8846 Validation Accuracy: 0.8948 Loss: 0.52251714 time: 0.0000s\n",
      "Epoch: 45000 Train Accuracy: 0.8846 Validation Accuracy: 0.8948 Loss: 0.52251714 time: 0.0157s\n",
      "Epoch: 46000 Train Accuracy: 0.8846 Validation Accuracy: 0.8948 Loss: 0.52251714 time: 0.0025s\n",
      "Epoch: 47000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0111s\n",
      "Epoch: 48000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0000s\n",
      "Epoch: 49000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0000s\n",
      "Epoch: 50000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0103s\n",
      "Epoch: 51000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0081s\n",
      "Epoch: 52000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0000s\n",
      "Epoch: 53000 Train Accuracy: 0.8924 Validation Accuracy: 0.8967 Loss: 0.51686043 time: 0.0000s\n",
      "Epoch: 54000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0061s\n",
      "Epoch: 55000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0000s\n",
      "Epoch: 56000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0000s\n",
      "Epoch: 57000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0035s\n",
      "Epoch: 58000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0000s\n",
      "Epoch: 59000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0046s\n",
      "Epoch: 60000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0000s\n",
      "Epoch: 61000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0000s\n",
      "Epoch: 62000 Train Accuracy: 0.8832 Validation Accuracy: 0.8875 Loss: 0.51583171 time: 0.0107s\n",
      "Epoch: 63000 Train Accuracy: 0.8813 Validation Accuracy: 0.8893 Loss: 0.51519483 time: 0.0040s\n",
      "Epoch: 64000 Train Accuracy: 0.8813 Validation Accuracy: 0.8893 Loss: 0.51519483 time: 0.0070s\n",
      "Epoch: 65000 Train Accuracy: 0.8813 Validation Accuracy: 0.8893 Loss: 0.51519483 time: 0.0070s\n",
      "Epoch: 66000 Train Accuracy: 0.8823 Validation Accuracy: 0.9207 Loss: 0.51504046 time: 0.0000s\n",
      "Epoch: 67000 Train Accuracy: 0.8864 Validation Accuracy: 0.8893 Loss: 0.51110697 time: 0.0015s\n",
      "Epoch: 68000 Train Accuracy: 0.8864 Validation Accuracy: 0.8893 Loss: 0.51110697 time: 0.0030s\n",
      "Epoch: 69000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0046s\n",
      "Epoch: 70000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0000s\n",
      "Epoch: 71000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0000s\n",
      "Epoch: 72000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0056s\n",
      "Epoch: 73000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0101s\n",
      "Epoch: 74000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0000s\n",
      "Epoch: 75000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0000s\n",
      "Epoch: 76000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0100s\n",
      "Epoch: 77000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0100s\n",
      "Epoch: 78000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0000s\n",
      "Epoch: 79000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0000s\n",
      "Epoch: 80000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0102s\n",
      "Epoch: 81000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0090s\n",
      "Epoch: 82000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0085s\n",
      "Epoch: 83000 Train Accuracy: 0.8897 Validation Accuracy: 0.8948 Loss: 0.50986475 time: 0.0111s\n",
      "Epoch: 84000 Train Accuracy: 0.8906 Validation Accuracy: 0.9077 Loss: 0.50754660 time: 0.0076s\n",
      "Epoch: 85000 Train Accuracy: 0.8906 Validation Accuracy: 0.9077 Loss: 0.50754660 time: 0.0000s\n",
      "Epoch: 86000 Train Accuracy: 0.8906 Validation Accuracy: 0.9077 Loss: 0.50754660 time: 0.0000s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87000 Train Accuracy: 0.8906 Validation Accuracy: 0.9077 Loss: 0.50754660 time: 0.0011s\n",
      "Epoch: 88000 Train Accuracy: 0.8906 Validation Accuracy: 0.9077 Loss: 0.50754660 time: 0.0091s\n",
      "Epoch: 89000 Train Accuracy: 0.8892 Validation Accuracy: 0.8893 Loss: 0.50559777 time: 0.0000s\n",
      "Epoch: 90000 Train Accuracy: 0.8892 Validation Accuracy: 0.8893 Loss: 0.50559777 time: 0.0031s\n",
      "Epoch: 91000 Train Accuracy: 0.8892 Validation Accuracy: 0.8893 Loss: 0.50559777 time: 0.0046s\n",
      "Epoch: 92000 Train Accuracy: 0.8892 Validation Accuracy: 0.8893 Loss: 0.50559777 time: 0.0075s\n",
      "Epoch: 93000 Train Accuracy: 0.8892 Validation Accuracy: 0.8893 Loss: 0.50559777 time: 0.0102s\n",
      "Epoch: 94000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0045s\n",
      "Epoch: 95000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0000s\n",
      "Epoch: 96000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0030s\n",
      "Epoch: 97000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0102s\n",
      "Epoch: 98000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0000s\n",
      "Epoch: 99000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0031s\n",
      "Epoch: 100000 Train Accuracy: 0.8994 Validation Accuracy: 0.8745 Loss: 0.50535721 time: 0.0000s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 392.1599s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(lr1[0].cpu().detach(),labels,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

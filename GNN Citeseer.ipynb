{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import normalize,toy_data,norm_embed,nmi_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution, InnerProduct\n",
    "from utils import norm_embed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, nfeat, nhid, ndim, mu0, sigma0, fixed):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(batch_size, nfeat, nhid, mu0, sigma0, scale=False)\n",
    "        self.fixed = fixed\n",
    "        if self.fixed:\n",
    "            self.embeddings = GraphConvolution(batch_size, nhid, 2*ndim, mu0, sigma0, scale=True)\n",
    "            self.reconstructions = InnerProduct(2*ndim)\n",
    "        else:\n",
    "            self.embeddings = GraphConvolution(batch_size, nhid, 4 * ndim, mu0, sigma0, scale=True)\n",
    "            self.reconstructions = InnerProduct(4*ndim)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "\n",
    "        if self.fixed:\n",
    "            mu = F.relu(self.reconstructions(x))\n",
    "            return mu, x\n",
    "        else:\n",
    "            lr1, lr2 = torch.chunk(x, chunks=2, dim=2)\n",
    "            mu = F.relu(self.reconstructions(lr1))\n",
    "            sigma = F.relu(self.reconstructions(lr2))\n",
    "            return mu, sigma, x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False,precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=426, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=500,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=10e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--ndim', type=int, default=2,\n",
    "                    help='Embeddings dimension.')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cite=False,cora=False,pub=False):\n",
    "    \n",
    "    if cite:\n",
    "        df1 = pd.read_csv('data/citeseer/citeseer.edges')\n",
    "        df2 = pd.read_csv('data/citeseer/citeseer.node_labels')\n",
    "        G = nx.from_pandas_edgelist(df1, 'u', 'v','weight',create_using=nx.DiGraph())\n",
    "        adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "        labels = df2.to_numpy()[:,-1].reshape(-1,1)\n",
    "        labels -= 1\n",
    "    \n",
    "    elif cora:\n",
    "        df1 = pd.read_csv('data/cora/cora.edges')\n",
    "        df2 = pd.read_csv('data/cora/cora.node_labels')\n",
    "        G = nx.from_pandas_edgelist(df1, 'u', 'v','weight',create_using=nx.DiGraph())\n",
    "        adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "        labels = df2.to_numpy()[:,-1].reshape(-1,1)\n",
    "        labels -= 1\n",
    "    \n",
    "    elif pub:\n",
    "        df1 = pd.read_csv('data/pubmed/pubmed.edges')\n",
    "        df2 = pd.read_csv('data/pubmed/pubmed.node_labels')\n",
    "        G = nx.from_pandas_edgelist(df1, 'u', 'v',create_using=nx.DiGraph())\n",
    "        adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "        \n",
    "    return adj_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, labels = load_data(cite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdApprox(adj,dim,relu=False):\n",
    "    adj = torch.FloatTensor(adj[0])\n",
    "    U, S, Vh = torch.linalg.svd(adj)\n",
    "    mu = torch.matmul(torch.matmul(U[:, :dim], torch.diag(S[:dim])), Vh[:dim, :])\n",
    "    \n",
    "    embedx = torch.matmul(U[:, :dim],torch.diag(torch.pow(S[:dim], 0.5)))\n",
    "    embedy = torch.transpose(torch.matmul(torch.diag(torch.pow(S[:dim], 0.5)),Vh[:dim, :]),0,1)\n",
    "    \n",
    "    criterion = torch.nn.GaussianNLLLoss()\n",
    "    if relu:\n",
    "        crt = torch.nn.ReLU()\n",
    "        mu = crt(mu)\n",
    "    mse = torch.nn.MSELoss()\n",
    "    mseloss = mse(torch.flatten(mu), torch.flatten(adj))\n",
    "    sig = torch.sqrt(mseloss)\n",
    "    sigma = sig * torch.ones(adj.shape)\n",
    "    loss = criterion(torch.flatten(adj), torch.flatten(mu), torch.flatten(torch.square(sigma)))\n",
    "    \n",
    "    return mu,sigma,loss.item(),embedx,embedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphNeuralNet(adj,dim,fixed=False,new=True,features=None,sig_fix=None):\n",
    "    \n",
    "    # Set the random seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        \n",
    "    args.ndim = dim\n",
    "    \n",
    "    adj_norm = normalize(adj)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj)).cuda()\n",
    "    \n",
    "    # loss function\n",
    "    criterion = torch.nn.GaussianNLLLoss()\n",
    "    \n",
    "    # NULL Model\n",
    "    mu0 = adj.mean()*torch.ones(adj.shape[1:]).cuda()\n",
    "    sigma0 = adj.std()*torch.ones(adj.shape[1:]).cuda()\n",
    "    with torch.no_grad():\n",
    "        loss0 = criterion(torch.flatten(adj), torch.flatten(mu0), torch.flatten(torch.square(sigma0)))\n",
    "    \n",
    "    if new:\n",
    "        if fixed:\n",
    "            #svd features\n",
    "            svd_mu,svd_sig,svd_loss,svdembedx,svdembedy = svdApprox(adj=adj.cpu(),dim=dim)\n",
    "            features = torch.cat((svdembedx,svdembedy),dim=1)\n",
    "        if not fixed:\n",
    "            sig_flex = torch.ones(features.shape).cuda()*torch.sqrt(sig_fix/dim).cuda()\n",
    "            features = torch.cat((features,sig_flex),dim=1).cuda()\n",
    "        features = features.unsqueeze(dim=0)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Model and optimizer  \n",
    "        \n",
    "    model = GNN(batch_size=adj.shape[0],\n",
    "                nfeat=adj.shape[1],\n",
    "                nhid=adj.shape[1],\n",
    "                ndim=args.ndim,\n",
    "                mu0=adj.mean(),\n",
    "                sigma0=adj.std(),\n",
    "                fixed=fixed)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        adj_norm = adj_norm.cuda()\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    \n",
    "    # NULL Model\n",
    "    mu0 = adj.mean()*torch.ones(adj.shape[1:]).cuda()\n",
    "    sigma0 = adj.std()*torch.ones(adj.shape[1:]).cuda()\n",
    "    with torch.no_grad():\n",
    "        loss0 = criterion(torch.flatten(adj), torch.flatten(mu0), torch.flatten(torch.square(sigma0)))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if fixed:\n",
    "            mu,lr = model(features, adj_norm)\n",
    "            with torch.no_grad():\n",
    "                mse = torch.nn.MSELoss()\n",
    "                mseloss = mse(torch.flatten(mu),torch.flatten(adj))\n",
    "                sig = torch.sqrt(mseloss)\n",
    "            sigma = sig * torch.ones(adj.shape,requires_grad=True).cuda()\n",
    "        else:\n",
    "            mu,sigma,lr = model(features, adj_norm)\n",
    "        \n",
    "        \n",
    "        loss = criterion(torch.flatten(adj), torch.flatten(mu), torch.flatten(torch.square(sigma))) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "            if fixed:\n",
    "                best_sig = sig\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_lr = lr\n",
    "                if fixed:\n",
    "                    best_sig = sig\n",
    "\n",
    "        if epoch == 0 or (epoch+1) % 100 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'loss: {:.8f}'.format(best_loss.item()))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    if fixed:\n",
    "        return mu,best_loss.item(),loss0,best_lr,best_sig\n",
    "    else:\n",
    "        return mu,best_loss.item(),loss0,best_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Sigma dim 128\n",
      "Epoch: 0001 loss: -3.64226055\n",
      "Epoch: 0100 loss: -4.09593439\n",
      "Epoch: 0200 loss: -4.62287331\n",
      "Epoch: 0300 loss: -4.62826586\n",
      "Epoch: 0400 loss: -4.63008642\n",
      "Epoch: 0500 loss: -4.63103914\n",
      "Optimization Finished!\n",
      "Total time elapsed: 122.2137s\n",
      "Epoch: 0001 loss: -4.63103914\n",
      "Epoch: 0100 loss: -4.63937473\n",
      "Epoch: 0200 loss: -4.63939285\n",
      "Epoch: 0300 loss: -4.63939285\n",
      "Epoch: 0400 loss: -4.63939285\n",
      "Epoch: 0500 loss: -4.63939285\n",
      "Optimization Finished!\n",
      "Total time elapsed: 123.7603s\n",
      "Flexible Sigma dim 128\n",
      "Epoch: 0001 loss: -4.63938665\n",
      "Epoch: 0100 loss: -6.78978777\n",
      "Epoch: 0200 loss: -6.79345655\n",
      "Epoch: 0300 loss: -6.79434299\n",
      "Epoch: 0400 loss: -6.79513073\n",
      "Epoch: 0500 loss: -6.79591084\n",
      "Optimization Finished!\n",
      "Total time elapsed: 124.0386s\n",
      "Epoch: 0001 loss: -6.79591084\n",
      "Epoch: 0100 loss: -6.79861546\n",
      "Epoch: 0200 loss: -6.80047035\n",
      "Epoch: 0300 loss: -6.80162811\n",
      "Epoch: 0400 loss: -6.80244064\n",
      "Epoch: 0500 loss: -6.80295801\n",
      "Optimization Finished!\n",
      "Total time elapsed: 122.5540s\n"
     ]
    }
   ],
   "source": [
    "for dim in [128]:\n",
    "    print(\"Fixed Sigma dim {}\".format(dim))\n",
    "\n",
    "    mu,loss,loss0,lr,sigma = GraphNeuralNet(adj=adj,dim=dim,fixed=True)\n",
    "    mu,loss,loss0,lr,sigma = GraphNeuralNet(adj=adj,dim=dim,fixed=True,new=False,features=lr.detach())\n",
    "\n",
    "    print(\"Flexible Sigma dim {}\".format(dim))\n",
    "    args.lr *= 0.1\n",
    "\n",
    "    mu,loss,loss0,lr = GraphNeuralNet(adj=adj,dim=dim,new=True,features=lr[0].detach(),sig_fix=sigma)\n",
    "    mu,loss,loss0,lr = GraphNeuralNet(adj=adj,dim=dim,new=False,features=lr.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1, lr2 = torch.chunk(lr, chunks=2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Train %\n",
    "\n",
    "train_percentage = .8\n",
    "    \n",
    "# Train set\n",
    "number_of_rows = lr1.shape[1]\n",
    "train_indices = np.random.choice(number_of_rows, size=int(train_percentage*number_of_rows), replace=False)\n",
    "val_indices = np.setdiff1d(np.arange(lr1.shape[1]),train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classify_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(classify_net, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        for i in range(4):\n",
    "            x1 = self.layer2(x)\n",
    "            x = x + x1\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(embed,labels,lr):\n",
    "\n",
    "    embed = torch.FloatTensor(embed)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    model = classify_net(embed.shape[1],32,7)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        embed = embed.cuda()\n",
    "        labels = labels.cuda()\n",
    "    \n",
    "\n",
    "    t_total = time.time()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr)\n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(100000):\n",
    "\n",
    "        t = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(embed)\n",
    "\n",
    "        train_output = output[train_indices,:]\n",
    "        train_labels = labels[train_indices,:]\n",
    "        \n",
    "        train_accuracy = torch.sum(torch.argmax(train_output,axis=1)==train_labels.reshape(1,-1))/train_labels.shape[0]\n",
    "\n",
    "        loss = criterion(output,labels.reshape(-1).long())\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Calculate Validation accuracy\n",
    "        with torch.no_grad():\n",
    "            val_output = output[val_indices,:]\n",
    "            val_labels = labels[val_indices,:]\n",
    "            val_accuracy = torch.sum(torch.argmax(val_output,axis=1)==val_labels.reshape(1,-1))/val_labels.shape[0]\n",
    "\n",
    "        # Print summary of training \n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_output = output\n",
    "            best_acc = train_accuracy\n",
    "            best_val_acc = val_accuracy\n",
    "            best_val_output = val_output\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_output = output\n",
    "                best_acc = train_accuracy\n",
    "                best_val_acc = val_accuracy\n",
    "                best_val_output = val_output\n",
    "\n",
    "        if epoch == 0 or (epoch+1) % 1000 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'Train Accuracy: {:.4f}'.format(best_acc.item()),\n",
    "                  'Validation Accuracy: {:.4f}'.format(best_val_acc.item()),\n",
    "                  'Loss: {:.8f}'.format(best_loss.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Train Accuracy: 0.0352 Validation Accuracy: 0.0413 Loss: 3.58457017 time: 0.0091s\n",
      "Epoch: 1000 Train Accuracy: 0.4910 Validation Accuracy: 0.4946 Loss: 1.28005040 time: 0.0031s\n",
      "Epoch: 2000 Train Accuracy: 0.6116 Validation Accuracy: 0.6141 Loss: 1.06029606 time: 0.0000s\n",
      "Epoch: 3000 Train Accuracy: 0.6775 Validation Accuracy: 0.6462 Loss: 0.94986045 time: 0.0011s\n",
      "Epoch: 4000 Train Accuracy: 0.7246 Validation Accuracy: 0.6953 Loss: 0.87724733 time: 0.0046s\n",
      "Epoch: 5000 Train Accuracy: 0.7392 Validation Accuracy: 0.7136 Loss: 0.82673550 time: 0.0040s\n",
      "Epoch: 6000 Train Accuracy: 0.7522 Validation Accuracy: 0.7381 Loss: 0.79148316 time: 0.0091s\n",
      "Epoch: 7000 Train Accuracy: 0.7675 Validation Accuracy: 0.7504 Loss: 0.76017016 time: 0.0000s\n",
      "Epoch: 8000 Train Accuracy: 0.7725 Validation Accuracy: 0.7688 Loss: 0.73753893 time: 0.0071s\n",
      "Epoch: 9000 Train Accuracy: 0.7790 Validation Accuracy: 0.7688 Loss: 0.72166973 time: 0.0011s\n",
      "Epoch: 10000 Train Accuracy: 0.7867 Validation Accuracy: 0.7626 Loss: 0.70173079 time: 0.0041s\n",
      "Epoch: 11000 Train Accuracy: 0.7890 Validation Accuracy: 0.7825 Loss: 0.68924397 time: 0.0101s\n",
      "Epoch: 12000 Train Accuracy: 0.8085 Validation Accuracy: 0.7841 Loss: 0.67682320 time: 0.0000s\n",
      "Epoch: 13000 Train Accuracy: 0.8031 Validation Accuracy: 0.7779 Loss: 0.66489345 time: 0.0101s\n",
      "Epoch: 14000 Train Accuracy: 0.8104 Validation Accuracy: 0.7887 Loss: 0.65667921 time: 0.0086s\n",
      "Epoch: 15000 Train Accuracy: 0.8085 Validation Accuracy: 0.7994 Loss: 0.65113002 time: 0.0000s\n",
      "Epoch: 16000 Train Accuracy: 0.8108 Validation Accuracy: 0.8009 Loss: 0.64362592 time: 0.0102s\n",
      "Epoch: 17000 Train Accuracy: 0.8146 Validation Accuracy: 0.7948 Loss: 0.63724935 time: 0.0020s\n",
      "Epoch: 18000 Train Accuracy: 0.8146 Validation Accuracy: 0.7948 Loss: 0.63724935 time: 0.0000s\n",
      "Epoch: 19000 Train Accuracy: 0.8142 Validation Accuracy: 0.7856 Loss: 0.63308442 time: 0.0000s\n",
      "Epoch: 20000 Train Accuracy: 0.8158 Validation Accuracy: 0.8116 Loss: 0.62924892 time: 0.0000s\n",
      "Epoch: 21000 Train Accuracy: 0.8100 Validation Accuracy: 0.8055 Loss: 0.62653029 time: 0.0000s\n",
      "Epoch: 22000 Train Accuracy: 0.8142 Validation Accuracy: 0.7856 Loss: 0.62415099 time: 0.0101s\n",
      "Epoch: 23000 Train Accuracy: 0.8100 Validation Accuracy: 0.8040 Loss: 0.61804700 time: 0.0075s\n",
      "Epoch: 24000 Train Accuracy: 0.8100 Validation Accuracy: 0.8040 Loss: 0.61804700 time: 0.0000s\n",
      "Epoch: 25000 Train Accuracy: 0.8142 Validation Accuracy: 0.8086 Loss: 0.61621892 time: 0.0070s\n",
      "Epoch: 26000 Train Accuracy: 0.8062 Validation Accuracy: 0.8101 Loss: 0.61297351 time: 0.0000s\n",
      "Epoch: 27000 Train Accuracy: 0.8188 Validation Accuracy: 0.8040 Loss: 0.61082643 time: 0.0045s\n",
      "Epoch: 28000 Train Accuracy: 0.8192 Validation Accuracy: 0.7933 Loss: 0.61057419 time: 0.0155s\n",
      "Epoch: 29000 Train Accuracy: 0.8181 Validation Accuracy: 0.8055 Loss: 0.60985249 time: 0.0025s\n",
      "Epoch: 30000 Train Accuracy: 0.8158 Validation Accuracy: 0.7994 Loss: 0.60728145 time: 0.0045s\n",
      "Epoch: 31000 Train Accuracy: 0.8204 Validation Accuracy: 0.8086 Loss: 0.60462874 time: 0.0000s\n",
      "Epoch: 32000 Train Accuracy: 0.8204 Validation Accuracy: 0.8086 Loss: 0.60462874 time: 0.0091s\n",
      "Epoch: 33000 Train Accuracy: 0.8204 Validation Accuracy: 0.8086 Loss: 0.60462874 time: 0.0046s\n",
      "Epoch: 34000 Train Accuracy: 0.8204 Validation Accuracy: 0.8086 Loss: 0.60462874 time: 0.0065s\n",
      "Epoch: 35000 Train Accuracy: 0.8204 Validation Accuracy: 0.8025 Loss: 0.60269684 time: 0.0000s\n",
      "Epoch: 36000 Train Accuracy: 0.8204 Validation Accuracy: 0.8025 Loss: 0.60269684 time: 0.0000s\n",
      "Epoch: 37000 Train Accuracy: 0.8181 Validation Accuracy: 0.8193 Loss: 0.60055077 time: 0.0000s\n",
      "Epoch: 38000 Train Accuracy: 0.8181 Validation Accuracy: 0.8193 Loss: 0.60055077 time: 0.0111s\n",
      "Epoch: 39000 Train Accuracy: 0.8154 Validation Accuracy: 0.8086 Loss: 0.59988141 time: 0.0101s\n",
      "Epoch: 40000 Train Accuracy: 0.8154 Validation Accuracy: 0.8086 Loss: 0.59988141 time: 0.0000s\n",
      "Epoch: 41000 Train Accuracy: 0.8131 Validation Accuracy: 0.8162 Loss: 0.59981841 time: 0.0000s\n",
      "Epoch: 42000 Train Accuracy: 0.8188 Validation Accuracy: 0.7994 Loss: 0.59877008 time: 0.0050s\n",
      "Epoch: 43000 Train Accuracy: 0.8254 Validation Accuracy: 0.8009 Loss: 0.59830171 time: 0.0000s\n",
      "Epoch: 44000 Train Accuracy: 0.8254 Validation Accuracy: 0.8009 Loss: 0.59830171 time: 0.0055s\n",
      "Epoch: 45000 Train Accuracy: 0.8254 Validation Accuracy: 0.8009 Loss: 0.59830171 time: 0.0000s\n",
      "Epoch: 46000 Train Accuracy: 0.8219 Validation Accuracy: 0.7734 Loss: 0.59821606 time: 0.0021s\n",
      "Epoch: 47000 Train Accuracy: 0.8219 Validation Accuracy: 0.7734 Loss: 0.59821606 time: 0.0076s\n",
      "Epoch: 48000 Train Accuracy: 0.8123 Validation Accuracy: 0.7994 Loss: 0.59749556 time: 0.0000s\n",
      "Epoch: 49000 Train Accuracy: 0.8208 Validation Accuracy: 0.8132 Loss: 0.59514827 time: 0.0000s\n",
      "Epoch: 50000 Train Accuracy: 0.8208 Validation Accuracy: 0.8132 Loss: 0.59514827 time: 0.0090s\n",
      "Epoch: 51000 Train Accuracy: 0.8208 Validation Accuracy: 0.8132 Loss: 0.59514827 time: 0.0000s\n",
      "Epoch: 52000 Train Accuracy: 0.8204 Validation Accuracy: 0.8086 Loss: 0.59448820 time: 0.0050s\n",
      "Epoch: 53000 Train Accuracy: 0.8204 Validation Accuracy: 0.8086 Loss: 0.59448820 time: 0.0100s\n",
      "Epoch: 54000 Train Accuracy: 0.8211 Validation Accuracy: 0.8070 Loss: 0.59202540 time: 0.0076s\n",
      "Epoch: 55000 Train Accuracy: 0.8211 Validation Accuracy: 0.8070 Loss: 0.59202540 time: 0.0000s\n",
      "Epoch: 56000 Train Accuracy: 0.8211 Validation Accuracy: 0.8070 Loss: 0.59202540 time: 0.0000s\n",
      "Epoch: 57000 Train Accuracy: 0.8154 Validation Accuracy: 0.7749 Loss: 0.59178782 time: 0.0000s\n",
      "Epoch: 58000 Train Accuracy: 0.8177 Validation Accuracy: 0.8239 Loss: 0.59047526 time: 0.0000s\n",
      "Epoch: 59000 Train Accuracy: 0.8177 Validation Accuracy: 0.8239 Loss: 0.59047526 time: 0.0000s\n",
      "Epoch: 60000 Train Accuracy: 0.8177 Validation Accuracy: 0.8239 Loss: 0.59047526 time: 0.0000s\n",
      "Epoch: 61000 Train Accuracy: 0.8177 Validation Accuracy: 0.8239 Loss: 0.59047526 time: 0.0030s\n",
      "Epoch: 62000 Train Accuracy: 0.8257 Validation Accuracy: 0.8070 Loss: 0.58885413 time: 0.0000s\n",
      "Epoch: 63000 Train Accuracy: 0.8200 Validation Accuracy: 0.8147 Loss: 0.58679891 time: 0.0000s\n",
      "Epoch: 64000 Train Accuracy: 0.8200 Validation Accuracy: 0.8147 Loss: 0.58679891 time: 0.0015s\n",
      "Epoch: 65000 Train Accuracy: 0.8200 Validation Accuracy: 0.8147 Loss: 0.58679891 time: 0.0000s\n",
      "Epoch: 66000 Train Accuracy: 0.8200 Validation Accuracy: 0.8147 Loss: 0.58679891 time: 0.0000s\n",
      "Epoch: 67000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0000s\n",
      "Epoch: 68000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0000s\n",
      "Epoch: 69000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0020s\n",
      "Epoch: 70000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0061s\n",
      "Epoch: 71000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0085s\n",
      "Epoch: 72000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0000s\n",
      "Epoch: 73000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0000s\n",
      "Epoch: 74000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0000s\n",
      "Epoch: 75000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0000s\n",
      "Epoch: 76000 Train Accuracy: 0.8307 Validation Accuracy: 0.8070 Loss: 0.58193070 time: 0.0042s\n",
      "Epoch: 77000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0000s\n",
      "Epoch: 78000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0100s\n",
      "Epoch: 79000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0000s\n",
      "Epoch: 80000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0040s\n",
      "Epoch: 81000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0000s\n",
      "Epoch: 82000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0000s\n",
      "Epoch: 83000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0000s\n",
      "Epoch: 84000 Train Accuracy: 0.8219 Validation Accuracy: 0.8055 Loss: 0.58118963 time: 0.0158s\n",
      "Epoch: 85000 Train Accuracy: 0.8250 Validation Accuracy: 0.8193 Loss: 0.58079165 time: 0.0000s\n",
      "Epoch: 86000 Train Accuracy: 0.8288 Validation Accuracy: 0.7994 Loss: 0.57951021 time: 0.0101s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87000 Train Accuracy: 0.8269 Validation Accuracy: 0.8101 Loss: 0.57795364 time: 0.0000s\n",
      "Epoch: 88000 Train Accuracy: 0.8269 Validation Accuracy: 0.8101 Loss: 0.57795364 time: 0.0000s\n",
      "Epoch: 89000 Train Accuracy: 0.8269 Validation Accuracy: 0.8101 Loss: 0.57795364 time: 0.0000s\n",
      "Epoch: 90000 Train Accuracy: 0.8269 Validation Accuracy: 0.8101 Loss: 0.57795364 time: 0.0000s\n",
      "Epoch: 91000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0000s\n",
      "Epoch: 92000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0000s\n",
      "Epoch: 93000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0046s\n",
      "Epoch: 94000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0000s\n",
      "Epoch: 95000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0000s\n",
      "Epoch: 96000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0131s\n",
      "Epoch: 97000 Train Accuracy: 0.8196 Validation Accuracy: 0.8208 Loss: 0.57706404 time: 0.0000s\n",
      "Epoch: 98000 Train Accuracy: 0.8250 Validation Accuracy: 0.8147 Loss: 0.57529211 time: 0.0080s\n",
      "Epoch: 99000 Train Accuracy: 0.8261 Validation Accuracy: 0.7917 Loss: 0.57473278 time: 0.0086s\n",
      "Epoch: 100000 Train Accuracy: 0.8261 Validation Accuracy: 0.7917 Loss: 0.57473278 time: 0.0085s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 374.7288s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(lr1[0].cpu().detach(),labels,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
